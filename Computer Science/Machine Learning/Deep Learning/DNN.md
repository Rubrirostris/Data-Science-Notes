## 神经网络

### 正向传播

$$
f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))
$$

神经网络是链式嵌套结构，输出值从前向后传播，每一层是一个线性层，加一个激活函数。线性层中包括w和bias，bias term可以使得我们分类的直线平移，增加了一个自由度，更加灵活，也可以视作激活函数的阈值；激活函数为线性函数加入了非线性，可以表征非线性数据。

在初始化时，通常采用随机数初始化，如果将权值初始化为 0 ，或者其他统一的常量，会导致后面的激活单元具有相同的值，所有的单元相同意味着它们都在计算同一特征，网络变得跟只有一个隐含层节点一样，这使得神经网络失去了学习不同特征的能力

- sigmoid 函数 ：$a=\frac{1}{1+e^{-z}}$

常用的激活函数，也是逻辑回归中的激活函数，当z趋近于无穷时，梯度趋近于0，可能会有梯度消失问题，在梯度下降时，收敛速度很慢

- tanh 函数（the hyperbolic tangent function，双曲正切函数）：$a = \frac{e^z - e^{-z}}{e^z + e^{-z}}$

效果通常好于sigmoid函数，值域在-1到1之间，有数据中心化的效果，当z趋近于无穷时，梯度趋近于0，可能会有梯度消失问题，在梯度下降时，收敛速度很慢

- ReLU 函数（the rectified linear unit，修正线性单元）：$a=max(0,z)$

当 z > 0 时，梯度始终为 1，从而提高神经网络基于梯度算法的运算速度，收敛速度远大于 sigmoid 和 tanh。然而当 z < 0 时，梯度一直为 0

Leaky ReLU（带泄漏的 ReLU）：$a=max(0.01z,z)$

Leaky ReLU 保证在 z < 0 的时候，梯度仍然不为 0。理论上来说，Leaky ReLU 有 ReLU 的所有优点，但在实际操作中没有证明总是好于 ReLU，因此不常用

![w600](https://i.loli.net/2021/08/02/GEeuqFxMY9noD2b.jpg)

### 反向传播

神经网络中根据链式法则，误差项反向传播，计算最终结果的误差对各个节点的导数，从而对整个网络的所有节点的参数值进行优化，反向传播通常通过计算图来计算导数

深度神经网络在反向传播过程中，多层的导数相乘，若每层的导数较大，则会发生梯度爆炸，最终结果很大。若每层导数较小，则会发生梯度消失，最终结果很小，常用的解决方法有更换激活函数、batchnorm、采用残差结构等。Relu激活函数的导数恒为1，不存在梯度消失和梯度爆炸的问题；Batchnorm对每一层的输出进行标准化，保证均值和方差一致；残差网络对上一层预测的残差进行训练，解决了梯度消失和爆炸的问题。

## 优化

### 基础方法

神经网络问题通常无法求出目标函数的闭式解，所以通常采用基于梯度的优化方法

- Batch Gradient Descend：最常用的梯度下降形式，即同时处理整个训练集；每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会比较慢

- Mini-batch Gradient Descend：每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。对整个训练集的一次遍历（称为一个 epoch）能做 mini-batch 个数个梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。这种方法在神经网络中最常用，适合并行计算


- Stochastic Gradient Descend：对每一个训练样本执行一次梯度下降，训练速度快


![image-20210320131646991](https://i.loli.net/2021/08/02/NZgAcEaq74b3r1e.png)

Batch梯度下降对所有样本做一次梯度下降，每次的迭代时间较长；Mini-batch梯度下降使用部分样本做梯度下降，兼容了模型的训练速度和梯度下降方向的准确性；随机梯度下降对每个样本进行一次梯度下降，速度最快，但噪声较大，且永远不会收敛

### 梯度下降的优化器

- 动量梯度下降（Gradient Descent with Momentum）：计算梯度的指数加权平均数，并利用该值来更新参数值


$$v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]}$$

$$v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]}$$

$$W^{[l]} := W^{[l]} - \alpha v_{dW^{[l]}}$$ 

$$b^{[l]} := b^{[l]} - \alpha v_{db^{[l]}}$$

![img](https://i.loli.net/2021/08/04/uFptSkiLynWCjzN.png)

使用动量梯度下降时，通过累加过去的梯度值来减少抵达最小值路径上的波动，加速了收敛，因此在横轴方向下降得更快，从而得到图中红色的曲线

- Adagrad(Adaptive Gradient Problem): 学习率调整算法

$$s_{dw} = s_{dw} + (dw)^2$$

$$s_{db} = s_{db} + (db)^2$$ 

$$w := w - \frac{\alpha}{\sqrt{s_{dw} + \epsilon}}dw$$

$$b := b - \frac{\alpha}{\sqrt{s_{db} + \epsilon}}db$$

- RMSProp（Root Mean Square Propagation，均方根传播）：在对梯度进行指数加权平均的基础上，引入平方和平方根。RMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率 α，从而加快算法学习速度


$$s_{dw} = \beta s_{dw} + (1 - \beta)(dw)^2$$

$$s_{db} = \beta s_{db} + (1 - \beta)(db)^2$$ 

$$w := w - \frac{\alpha}{\sqrt{s_{dw} + \epsilon}}dw$$

$$b := b - \frac{\alpha}{\sqrt{s_{db} + \epsilon}}db$$

![img](https://i.loli.net/2021/08/04/GZaKDHj4Ir9LVCF.png)

- Adam 优化算法（Adaptive Moment Estimation，自适应矩估计）：将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果。


$$v_{dW} = \beta_1 v_{dW} + (1 - \beta_1) dW$$ 

$$v_{db} = \beta_1 v_{db} + (1 - \beta_1) db$$

$$s_{dW} = \beta_2 s_{dW} + (1 - \beta_2) {(dW)}^2$$ 

$$s_{db} = \beta_2 s_{db} + (1 - \beta_2) {(db)}^2$$

$$v^{corrected}_{dW} = \frac{v_{dW}}{1-{\beta_1}^t}$$ 

$$v^{corrected}_{db} = \frac{v_{db}}{1-{\beta_1}^t}$$

$$s^{corrected}_{dW} = \frac{s_{dW}}{1-{\beta_2}^t}$$ 

$$s^{corrected}_{db} = \frac{s_{db}}{1-{\beta_2}^t}$$

$$W := W - \frac{\alpha}{{\sqrt{s^{corrected}_{dW}} + \epsilon}}v^{corrected}_{dW}$$

$$b := b - \frac{\alpha}{{\sqrt{s^{corrected}_{db}} + \epsilon}}v^{corrected}_{db}$$

### 学习率

学习率过小，学习速度较慢，较多时间才能获得最优解

学习率过大，可能会错过最优解，梯度下降过程不收敛

### 优化问题

在微积分中，通常情况下我们判断一个驻点是否是极值点使用二阶导数，而针对多维数据，二阶导数是Hessian矩阵，Hessian矩阵正定，驻点为极小值点；二阶导数负定，驻点为极大值点，二阶导数不定，驻点为鞍点

**鞍点（saddle）**是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是下图所示的鞍点，而非局部最小值。减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。

![img](https://i.loli.net/2021/08/04/5FyWjSG1DvLVph6.png)

在低维空间中局部最小值最常见，在高维空间中鞍点最常见。局部最小值的Hessian矩阵特征值全为正，鞍点的Hessian矩阵特征值有正有负，这也能解释为什么在高维空间中鞍点常见。

### 超参数搜索

Random search：随机搜索，在参数空间中随机选择参数组合，计算效率相比网格搜索更高

Grid search：网格搜索，M个参数，每个参数有N个候选值，尝试$M^N$种不同的参数组合

## 正则化

适用于传统机器学习算法解决过拟合问题的方法同样适用于深度学习，例如L1 L2正则化，在深度学习中，常常使用weight decay方法做正则化，等同L2正则

除此之外深层神经网络还可以采用Dropout、BatchNorm、Early stopping、Data Augmentation等方法解决过拟合

- Dropout: 神经网络的每个单元都被赋予在计算中被暂时忽略的概率p。超参数p称为丢失率，通常将其默认值设置为0.5。然后，在每次迭代中，根据指定的概率随机选择丢弃的神经元。因此，每次训练会使用较小的神经网络，这种方法减小了模型的复杂度。

  训练模型阶段：在训练网络的每个单元都要添加一道概率流程，以一定概率舍弃某个神经元，同时对权重进行缩放，乘概率 $\frac{1}{1-p}$

  测试模型阶段：用所有神经元参与计算

- BatchNorm：在神经网络的训练中，为了减少梯度消失和梯度爆炸的情况，通常在全连接层后加入批标准化层，减去均值除以方差，使得求梯度的值在0附近；批标准化同时也具有正则化的作用，见笑了数据的不稳定性

- Early stopping：在用梯度下降法的过程中，在模型开始过拟合之前就中断学习过程，通常通过训练集误差和验证集误差来判断


- 数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集，数据扩增等于加入了一些噪音在原始数据中，会带来更好的泛化性能，使用数据扩增时，通常在前几个epoch会发生测试集准确率高于训练集准确率的情况

