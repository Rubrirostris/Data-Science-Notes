## 卷积运算

### 二维卷积

卷积运算的求解过程是从左到右，由上到下，每次在原始图片矩阵中取与滤波器同等大小的一部分，每一部分中的值与滤波器中的值对应相乘后求和，将结果组成一个矩阵。

假设输入图片的大小为$n×n$，而滤波器的大小为$f×f$，则卷积后的输出图片大小为$(n−f+1)×(n−f+1)$

![img](https://i.loli.net/2021/08/02/oAjQbqfurwDtsan.png)

普通卷积运算存在的问题是：每次卷积运算后，输出图片的尺寸缩小；原始图片的角落、边缘区像素点在输出中采用较少，输出图片丢失边缘位置的很多信息。为了解决这个问题，在进行卷积操作前，对原始图片在边界上进行**填充（Padding）**，以增加矩阵的大小。通常将 0 作为填充值。

设每个方向扩展像素点数量为$p$，则填充后原始图片的大小为$(n+2p)×(n+2p)$，滤波器大小保持$f×f$不变，则输出图片大小为 $(n+2p−f+1)×(n+2p−f+1)$

卷积过程中，有时需要通过填充来避免信息损失，有时也需要通过设置**步长（Stride）**来压缩一部分信息。
$$
⌊\frac{n+2p−f}{s}+1⌋×⌊\frac{n+2p−f}{s}+1⌋
$$
卷积操作的原理

- **参数共享（Parameter sharing）**：特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。
- **稀疏连接（Sparsity of connections）**：在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值。

### 三维卷积

如果我们想要对三通道的 RGB 图片进行卷积运算，那么其对应的滤波器组也同样是三通道的。过程是将每个单通道（R，G，B）与对应的滤波器进行卷积运算求和，然后再将三个通道的和相加，将 27 个乘积的和作为输出图片的一个像素值。

![img](https://i.loli.net/2021/08/02/NkWiVebrlOf4dQ2.png)

如果想同时检测垂直和水平边缘，或者更多的边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。设输入图片的尺寸为$n×n×n_c$（$n_c$为通道数），滤波器尺寸为$f×f×n_c$，则卷积后的输出图片尺寸为$ (n−f+1)×(n−f+1)×n′_c$，$n′_c$为滤波器组的个数。

![img](https://i.loli.net/2021/08/02/XjicpDbhByIVZeo.png)

与之前的卷积过程相比较，卷积神经网络的单层结构多了激活函数和偏移量。随着神经网络计算深度不断加深，图片的高度和宽度 $n_H$、$n_W$一般逐渐减小，而$n_c$在增加。

对于一个 3x3x3 的滤波器，包括偏移量 bb在内共有 28 个参数。不论输入的图片有多大，用这一个滤波器来提取特征时，参数始终都是 28 个，固定不变。即**选定滤波器组后，参数的数目与输入图片的尺寸无关**。因此，卷积神经网络的参数相较于标准神经网络来说要少得多。这是 CNN 的优点之一。

### 池化层

卷积网络也经常使用池化层来缩减模型的大小，提高计算速度，同时提高所提取特征的鲁棒性

**最大池化（Max Pooling）**：将输入拆分成不同的区域，输出的每个元素都是对应区域中元素的最大值

**平均池化（Average Pooling）**：从取某个区域的最大值改为求这个区域的平均值

![img](https://i.loli.net/2021/08/02/AkOG1FI9uYNVSiX.png)

## 经典模型

### LeNet

- LeNet-5 针对灰度图像而训练，因此输入图片的通道数为 1。
- 该模型总共包含了约 6 万个参数，远少于标准神经网络所需。
- 典型的 LeNet-5 结构包含卷积层（CONV layer），池化层（POOL layer）和全连接层（FC layer），排列顺序一般为 CONV layer->POOL layer->CONV layer->POOL layer->FC layer->FC layer->OUTPUT layer。一个或多个卷积层后面跟着一个池化层的模式至今仍十分常用。
- 当 LeNet-5模型被提出时，其池化层使用的是平均池化，而且各层激活函数一般选用 Sigmoid 和 tanh。现在，我们可以根据需要，做出改进，使用最大池化并选用 ReLU 作为激活函数。

![img](https://i.loli.net/2021/08/02/IJcUaSYs4uQ1vVl.png)

### AlexNet

- AlexNet 模型与 LeNet-5 模型类似，但是更复杂，包含约 6000 万个参数。另外，AlexNet 模型使用了 ReLU 函数。同时pooling layer使用了max pooling。同时加入了dropout进行正则化（全链接层之后）在训练过程中使用了数据增强。
- 当用于训练图像和数据集时，AlexNet 能够处理非常相似的基本构造模块，这些模块往往包含大量的隐藏单元或数据。

![img](https://i.loli.net/2021/08/02/ZSzqvTtModxWU3K.png)

### VGG-16

- VGG 又称 VGG-16 网络，“16”指网络中包含 16 个卷积层和全连接层。
- 超参数较少，只需要专注于构建卷积层。
- VGG使用可重复使用的卷积块来构建深度卷积神经网络。结构不复杂且规整，在每一组卷积层进行滤波器翻倍操作。
- VGG 需要训练的特征数量巨大，包含多达约 1.38 亿个参数。

![img](https://i.loli.net/2021/08/02/Cl3Mdkr1HoXfe5z.png)

### ResNets

在理论上，随着网络深度的增加，性能应该越来越好。但实际上，对于一个普通网络，随着神经网络层数增加，训练错误会先减少，然后开始增多。但残差网络的训练效果显示，即使网络再深，其在训练集上的表现也会越来越好。

对于残差网络来说，加入了跳跃连接部分，发生梯度消失时，额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。



![img](https://i.loli.net/2021/08/02/GeLMhaPE9muIjgd.png)

### Inception

在之前的卷积网络中，我们只能选择单一尺寸和类型的滤波器。而 **Inception 网络的作用**即是代替人工来确定卷积层中的滤波器尺寸与类型，或者确定是否需要创建卷积层或池化层

下图中的inception块从4个不同路径抽取信息，然后在输出通道合并。

![img](https://i.loli.net/2021/11/20/pyQ6Z7HBngEfkdI.png)

![img](https://i.loli.net/2021/08/02/8wsLWXR1jaAvgKC.png)

