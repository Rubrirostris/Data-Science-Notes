## Description

Do you love building data pipelines? Are you excited by the opportunity to design tools and infrastructure  needed to analyze large volumes of data? Do you want to help solve big  data warehousing problems, and partner with stakeholders to understand  how to best design and implement cutting edge data solutions that  provide answers to key business questions? Do you want to be a part of a fast-paced environment and contribute to one of the most visited sites  on the Internet?

If this describes you, consider joining us as an intern in the summer of 2022. Amazon is looking for a data engineer  intern to join one our many lines of business. Amazon interns have the  opportunity to work alongside the industry’s brightest engineers who  innovate everyday on behalf of our customers. You will be matched to a  manager and a mentor. You will have the opportunity to impact the  evolution of Amazon technology as well as lead mission critical projects early in your career. Your work will contribute to solving some of the  most complex technical challenges in the company.

In addition to  working on an impactful project, you will have the opportunity to engage with Amazonians for both personal and professional development, expand  your network, and participate in fun activities with other interns  throughout the summer. No matter the location of your internship, we  give you the tools to own your summer and learn in a real world setting.

Come chart your own path at Amazon.

Amazon internships are full-time (40 hours/week) for 12 consecutive weeks with start dates in May - July 2022. Applicants should have at a minimum one quarter/semester remaining after their internship concludes.

### Responsibilities
As a data engineer intern, you will/may:
· Design, implement, and automate deployment of our distributed system  for collecting and processing log events from multiple sources
· Design data schema and operate internal data warehouses and SQL/NoSQL database systems
· Own the design, development, and maintenance of ongoing metrics,  reports, analyses, and dashboards that engineers, analysts, and data scientists use to drive key business decisions
· Monitor and troubleshoot operational or data issues in the data pipelines
· Drive architectural plans and implementation for future data storage, reporting, and analytic solutions
· Develop code based automated data pipelines able to process millions of data points
· Improve database and data warehouse performance by tuning inefficient queries
· Work collaboratively with Business Analysts, Data Scientists, and other internal partners to identify opportunities/problems
· Provide assistance to the team with troubleshooting, researching the root cause, and thoroughly resolving defects in the event of a problem

### BASIC QUALIFICATIONS

· Currently enrolled in or will receive a Bachelor’s in Computer Science, Computer Engineering,Information Management, Information Systems, or an equivalent technical discipline with a conferral date between September 2022 – August 2024
· Experience with data mining and data transformation
· Experience with database and/or data warehouse solutions
· Experience building data pipelines or automated ETL processes
· Experience with SQL
· Experience with one or more scripting language (e.g., Python, KornShell)

### PREFERRED QUALIFICATIONS

· Enrolled in a Master’s Degree or advanced technical degree
· Previous technical internship(s), if applicable
· Can articulate the basic differences between datatypes (e.g. JSON/NoSQL, relational)
· Familiar with the basic implications of different implementation decisions (e.g., distributed processing, parallel processing)
· Understand the basics of designing and implementing a data schema (e.g., normalization, relational model vs dimensional model)
· Experience building code based on data pipelines able to process big datasets
· Knowledge of writing and optimizing SQL queries with large-scale, complex datasets
· Experience with big data processing technology (e.g., Hadoop or  ApacheSpark), data warehouse technical architecture, infrastructure components, and reporting/analytic tools and environments
· Experience with data visualization software (e.g., AWS QuickSight or Tableau)

## Leadership Principle

- Customer Obsession: Leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. Although leaders pay attention to competitors, they obsess over customers. (Google machine learning time period)

- Ownership: Leaders are owners. They think long term and don’t sacrifice long-term value for short-term results. They act on behalf of the entire company, beyond just their own team. They never say “that’s not my job." (Google clients visit)

- Invent and Simplify: Leaders expect and require innovation and invention from their teams and always find ways to simplify. They are externally aware, look for new ideas from everywhere, and are not limited by “not invented here." As we do new things, we accept that we may be misunderstood for long periods of time. (Google hypothesis testing; Google data pipeline)

- Are Right, A Lot: Leaders are right a lot. They have strong judgment and good instincts. They seek diverse perspectives and work to disconfirm their beliefs. (Traffic congestion prediction)

- Learn and Be Curious: Leaders are never done learning and always seek to improve themselves. They are curious about new possibilities and act to explore them. (Short video recommender system; DiDi data pipelines)

- Hire and Develop the Best: Leaders raise the performance bar with every hire and promotion. They recognize exceptional talent, and willingly move them throughout the organization. Leaders develop leaders and take seriously their role in coaching others. We work on behalf of our people to invent mechanisms for development like Career Choice. (Traffic congestion prediction)

- Insist on the Highest Standards: Leaders have relentlessly high standards — many people may think these standards are unreasonably high. Leaders are continually raising the bar and drive their teams to deliver high quality products, services, and processes. Leaders ensure that defects do not get sent down the line and that problems are fixed so they stay fixed. (Google hypothesis testing)

- Think Big: Thinking small is a self-fulfilling prophecy. Leaders create and communicate a bold direction that inspires results. They think differently and look around corners for ways to serve customers. (Short video recommender system)

- Bias for Action: Speed matters in business. Many decisions and actions are reversible and do not need extensive study. We value calculated risk taking. (Google clients visit)

- Frugality: Accomplish more with less. Constraints breed resourcefulness, self-sufficiency, and invention. There are no extra points for growing headcount, budget size, or fixed expense. (DiDi data pipeline; Google hypothesis testing)

- Earn Trust: Leaders listen attentively, speak candidly, and treat others respectfully. They are vocally self-critical, even when doing so is awkward or embarrassing. Leaders do not believe their or their team’s body odor smells of perfume. They benchmark themselves and their teams against the best. (Traffic Congestion Prediction; Google machine learning time period)

- Dive Deep: Leaders operate at all levels, stay connected to the details, audit frequently, and are skeptical when metrics and anecdote differ. No task is beneath them. (Machine Learning Research; Short video recommender system)

- Have Backbone; Disagree and Commit: Leaders are obligated to respectfully challenge decisions when they disagree, even when doing so is uncomfortable or exhausting. Leaders have conviction and are tenacious. They do not compromise for the sake of social cohesion. Once a decision is determined, they commit wholly. (Traffic congestion prediction; Google machine learning time period)

- Deliver Results: Leaders focus on the key inputs for their business and deliver them with the right quality and in a timely fashion. Despite setbacks, they rise to the occasion and never settle. (DiDi data pipelines)

- Strive to be Earth's Best Employer: Leaders work every day to create a safer, more productive, higher performing, more diverse, and more just work environment. They lead with empathy, have fun at work, and make it easy for others to have fun. Leaders ask themselves: Are my fellow employees growing? Are they empowered? Are they ready for what's next? Leaders have a vision for and commitment to their employees' personal success, whether that be at Amazon or elsewhere. (Class monitor)

- Success and Scale Bring Broad Responsibility: We started in a garage, but we're not there anymore. We are big, we impact the world, and we are far from perfect. We must be humble and thoughtful about even the secondary effects of our actions. Our local communities, planet, and future generations need us to be better every day. We must begin each day with a determination to make better, do better, and be better for our customers, our employees, our partners, and the world at large. And we must end every day knowing we can do even more tomorrow. Leaders create more than they consume and always leave things better than how they found them. (All the internship and research)

## Behavior

- Why Amazon?

Firstly, Amazon is a top internet company. The mission of Amazon is to continually raise the bar of the customer experience by using the internet and technology. The vision of company inspires me. I'm also an Amazon customer, and I hope join Amazon to help improve the customer experienced and bring broad responsibility to the society. Secondly, the work environment attracts me a lot. Amazon strives to be the best employer and has an open, diverse and productive work environment. I think working in such a company is happy and productive. Finally, there are many talents working for Amazon, and I think working with these people could help me grow up fast.

- Why choose this position?

The job I have applied is the data engineer intern. When I was an undergraduate student in Tsinghua University, I found my passion in Data Science. And I enrolled in many programming and statistics courses to strengthen my technical skills. I also did some internships in Tech companies like DiDi and Google. However, when I were doing internship, I found the most challenging and demanding part of data science is to build data pipelines. Without a stable data pipeline, you cannot build a good machine learning model. Therefore, I hope to be a data engineer in the future. And besides that, I also want to do more engineering work, compared to data analysis, engineering is more visible and predictable. I felt sense of achievement when doing such kind of work. 

- Introduce yourself

My name is Jingxiang, and I'm currently a master student in Data Science in Columbia University. I did my undergraduate in Tsinghua University. During my undergraduate study, I found my passion in data science and data engineering through some courses projects. So I enrolled in many computer science courses to strengthen my technical skills. Beyond my major, I also did internships in tech companies like DiDi and Google to learn more about the industry. With solid background in coding, I am able to build efficient data pipelines and use machine learning techniques to help make data-driven decision. I find it is an interesting and proud thing to successfully build a large scale data pipeline to help make business impacts. Therefore, I applied for data engineer intern in Amazon. And after internship, I hope to convert to a full time employee and continue my data engineering career.

- Introduce a data pipeline you built (Most challenging project)

Invent and simplify; Insist on the highest standards

The situation was that I was an intern in Google China. Our team was to help local clothing companies do international business. We helped our clients published ads on Google search. My task was to build a data pipeline of fashion keywords frequency on social media and helped our clients find the trending items on social media. The challenge was that I need to build the whole pipeline and data warehouse on my own. In order to do this, I did my research and follow the instruction step by step. Firstly, I used a 3rd party web crawling tool named Brandwatch to query keywords frequency on different social media sources such as Twitter, Youtube, and Facebook. Secondly, I wrote a python script with Brandwatch's API to extract fashion keywords frequency. The most annoying part is that the API is not that stable and sometimes server errors might occur. And I tried a new way to build up several retry modules using python decorator to solve this problem. For data warehouse building, although it's simple, I still used star schema to design my data warehouse. There were two tables, one was the fact table recording the keywords frequency from each social media source each day, the other was the dimensional table recording the keywords categories and tags. Finally, I published and automated the entire pipeline on Google cloud compute engine. The results were that I maintained a 10k keywords data warehouse and the data update once a week automatically. That is a good warehouse for machine learning job and visualization.

Learn and be curious; Deliver results

The situation was that I was a Data Engineer Intern in DiDi. DiDi is Chinese biggest online car hiring company, it's like uber in United States. Our team is in charge of route recommender system. There is one strategy for route recommendation, which is to recommend the most frequent route for a given origin and destination. The task for me was to improve the data pipelines for frequent routes mining. And there were two shortcomings for the system, one was the efficiency and missing data, the other was the scale of the pipelines. The pipelines were only for top 50 cities before. The biggest challenge for me was that I was new to Scala and Spark. My action was that firstly I met with my mentor asking for some studying resources to learn about these tech stacks. I even spent some of my personal time for study because I hoped to start my work as soon as possible. Secondly, after knowing some basic knowledge, I started to look out the docs for the data dependency of the system. I figured out there was one useless dataset. Without this dataset, the efficiency could be improved. So I rebuilt a new pipeline with Scala and Spark without this redundant dataset. Another task was to promote the top 50 cities to the whole country. In order to improve the efficiency, I split the cities into 10 groups and built several shell scripts to mine data of different cities  sets in parallel. That significantly increase the efficiency. The result was that I built a data pipeline for frequent route mining for the whole country. The mining time decreased from 2 days to 1 day and the recall ratio of frequent route increased by 13%.

- Failure (Customer obsessed; Earn trust)

Customer obsessed; Earn trust (Negative feedback)

The situation was that I was an intern in Google China this summer. The task was to build up a dashboard reflecting the market trends to help our clients make stock preparation. In my first a few weeks, I spent most of my time in building data pipelines and dashboards. However, I ignore the needs of clients. So in my midterm evaluation, my manager gave me a negative feedback on customer obsession. After that, I felt the most important thing is to know our clients. And I schedule several meetings with our clients and sales people to know about their business. And I knew that the biggest challenge for our clients is what to sell. Therefore, I focused on finding the trending items for our clients in the remaining internship. Finally, my dashboard was a great tool for clients to know the market and decide what to sell in the future. And finding one single item like tiktok leggings could help our clients generate 200k revenue. And I helped our team earn clients' trust. The learning is that I should always keep customers in mind. If there is some negative feedback, ask for the reasons and correct the mistake immediately to minimize the negative influence.

- Most challenging project (Insist on the highest standards; Ownership; Learn and be curious)

Insist on the highest standards; Learn and be curious; Dive deep

The situation was that I was a research assistant in Tsinghua University. Our team focused on causal inference. My task was to improve current state-of-the-art causal model. The challenge was that I never heard about causal inference before. Firstly, I did some literature review about causal inference research. I found that most of the models represent a linear causal relationship. It is limited for nonlinear causal discovery. After meeting with my supervisor, I designed a structural equation model where the causal relationship described by Gaussian process. In order to model some unobserved variables, I also introduced confounders into the model. My model could find the nonlinear causal relationship between variables and confounders. This step is the most challenging one. Actually, I spent a lot of time proving my model could work and translated my model to an optimization problem. In order to solve the optimization problem, I chose Monte Carlo simulations and Expectation Maximization algorithm due to the unobservable confounders. I also built a python causal inference toolbox of my model. Finally, I evaluated my model in synthetic dataset and real-case dataset. The model showed good performance in both fitting and causal discovery. The toolbox was a great tool to find the causality between different time series and it's quite suitable for spatial-temporal data causal discovery like subway stations passenger flow causal discovery.

- Miss deadlines (Customer Obsessed; Deliver Results; Ownership; Dive deep)

Learn and be curious; Deliver results;

The situation was that I was a research assistant in Tsinghua University. And my task was to design a new model for causal inference and submit my work to a top machine learning conference. Before my research, my mentor and I set up a goal to submit my paper on ICML 2021, and the deadline was Feb, 2021. Because I was not familiar with causal inference research. Therefore, firstly I spent a lot of time doing literature review. The causal inference is quite an interesting area in machine learning research. However, I spent too much time learning about it. Then I started to design my own model, but I encountered some problems in parameter estimating method. The time was limited. In order to do it, I often asked my supervisor for advice. And that significantly speed up my research. After solving the problem with my supervisor, I didn't have enough time to finish my paper. And I didn't make it to submit my paper on ICML. Then I asked my supervisor to extend the deadline and resubmit the paper on NIPS 2021. Finally, I finished all the experiments and writing and submitted my well-defined paper to NIPS. The takeaway of this failure was that building up a new thing in research is difficult. So I should communicate more with my supervisor and peers to learn new thing. And I should keep up with the deadline and move fast. If missing a deadline, talk with the stakeholder about it to reduce negative impacts of it and finish it as soon as possible.

- Conflict (Customer Obsessed; Earn trust)

Are right, a lot; Earn trust; Have Backbone, Disagree and Commit

The situation was that I was doing a data contest by China Computer Federation. My work was to predict the traffic congestion level by relevant data. Our team had 5 people. And when we met firstly, I told them that we could use traditional tree based model for prediction. One of my teammates said we should considered time series models instead. I disagreed with him because in this project, the time steps were short, so the time series might not have the best performance. However, he insisted his opinion. So I decided to try both models and build a prototype to compare the results. Finally, it showed that my model performance was better than his. Then I earned his trust and he listened to my suggestion. Our team used tree-based model to make prediction and realize good performance. The best way to persuade others is to show them your result and prove it could work better.

Customer Obsessed; Earn trust;

The situation was that I was a data scientist intern in Google China. Our team was to help local clothing company do international business. My task was to build a machine learning model to predict the future fashion keywords frequency. And I had conflicts with a business analyst. She didn't know too much about machine learning and time series prediction. And she wanted to predict for future half year or even one year. However, there were some limits for time series prediction that the accuracy will decrease with the prediction time steps increase. In order to solve this conflict. I firstly met with sales team to know the customers needs. The stock preparation cycle is about 2 weeks to 1 month. Therefore, if I predicted for future one month trending items, then it would be helpful and enough for our clients to make stock preparation in advance. And I also tested the prediction error for future half year. And I found that the error increased but for future three months, the prediction error are acceptable. After doing these, I met with the business analyst and told her the results. She totally agreed with me and said we only predicted for future three months and showed them on dashboard.

- Take a risk, or do not have much time, to make a decision (Bias for action; Ownership)

Customer Obsessed; Bias for action;

The situation was that I was an intern in Google, and our team is to help local clothing company do international business. I helped our clients build up a dashboard reflecting the market trends. And my colleague and I visited clients to present the result. The task for me was to present the dashboard to clients and collect feedback. Our clients were satisfied with my dashboard. And they had one further more question that if it is possible to use some item pictures to predict the future sales. My colleague didn't know too much about machine learning and deep learning. Although it was not my round to answer the question, I took initiatives to answer that. Firstly, I said that it was possible to extract information from picture using some computer vision techniques like convolutional neural nets to extract features from item pictures. Then we could train machine learning models using these features to predict the future sales. However, there are many features influencing the sales, not only item features, but some features about customers and some confounders like weather, season. So the prediction might not be useful. Our clients were satisfied with my answer. So in this case, I took action and risk to answer the clients' questions and helped our team maintain good relationship with clients.

- Sacrifice short term for long term (Think big)

Think big; Dive deep

The situation was that I participated in a data contest this spring about short video recommender system. The contest was to predict the occurrence probability of different user actions such as like, comment. The training set was the user actions in the last two weeks and some user information and item information. There was a baseline model provided by the contest, and that was the Wide and Deep model. I found that if I fine tune the wide and deep model, it could realize a good performance in prediction. However, I won't learn too much about recommender system. Considering the long term gains, I decided to dive deep to learn more about the recommender system from the traditional collaborate filtering algorithm, matrix decomposition algorithm to the state-of-the-art DeepFM model. And I also summarized my learning and made a GitHub repo to take notes. After that, I started to participate in this contest. Firstly, I did some exploratory data analysis and feature engineering. Then I built three kinds of model, including LighGBM, Wide and Deep and DeepFM to make prediction. Finally, I combined the results of three models. The results showed that my model realized an AUC score of 0.67, ranking top 15% among all teams.

- Tough decision (Bias for action)

Bias for action

The situation was that in my junior year I was about to apply for the master study. And I had two choices, one was to continue studying my undergraduate major, the other was to switch my career to data science. At that time I found I like using data to make impacts. After serious consideration, I decided to follow my heart and switched my career to data science in tech company. After making my decision, my task was to improve my background about data science. And I started to prepare for it immediately. Firstly, I enrolled in many programming courses to strengthen my technical skills. Secondly, I did two internships in tech company to learn more about industry. Then I continued my graduate study in Data Science at Columbia University. The results were that I have learned a lot about data science. Now I'm fully prepared to start my data engineer and data science career. The learning is that when making a tough decision, follow your heart and then start to do it as soon as possible.

- Tell me a time when you came up with a simple solution to solve a problem

Invent and Simplify; Deliver results;

The situation was that I was an intern in Google China. Our team was to help local clothing companies do international business. We helped our clients publish ads on Google search. The biggest challenge for our customer is what to sell. In the past, our team used google trends to learn about the markets. However, we hope to be faster to know the market trends. There is a hypothesis that fashion trends on social media is ahead of google search. Because a fashion trend usally begins with social media celebrities. My task was to validate this hypothesis. If validated, we could be faster to know the trends with social media data and our following work would be meaningful. Firstly, I randomly select 30 fashion rising keywords in 2020 and see the peak in Google trends and social media trends. If social media is ahead of google search, the hypothesis was validated. However, there were two problems of this way, one was that the peak cannot represent the arrival of a fashion trend, the other was that 30 human selected keywords were not representative. I did literature review and found that there was a simple hypothesis testing method called Granger Causality Test. This method is suitable for determining whether one time series is useful in forecasting another. So it's quite suitable to our problem. And then I performed Granger Causality tests for given fashion keywords. Finally, the hypothesis test showed that there were 41% fashion keywords trends on social media were ahead of google search trends. The result of hypothesis test meant we could use social media data source to find trending words faster for our clients. Therefore, our following work would be meaningful.

- Tell me a time when you took on something significant outside your area of responsibility

Ownership; Deliver results; Bias for action

The situation was that I was a data engineer intern at DiDi. Our team was in charge of route recommender system. My task was to improve the efficiency of frequent route mining engine. One day there was an emergent task to calculate the recall ratio of frequent route in all orders. The task was important because we could make future plans based on that. However, all of my colleagues were busy doing their own tasks and did not have spare time to do this. At this point, I took initiative to help with this task. Firstly, I referred to the previous Scala code of the problem, then I started to do this need. Finally I gave the result to product manager on time and continued to do my own project. The results showed that the recall ratio increased with our frequent route mining strategy. So the next step was to improve the process.

- Your strength

I think my strength is that I'm a quick learner. My undergraduate major was Environmental Engineering. It's not that data heavy. But when I found my passion through some course projects. I took initiatives to enroll in many statistics and programming courses and did internships in tech companies to strengthen my data skills. Now I think I have strong data science expertise to work as a data scientist.

- Your weakness

I think my weakness is trying to do everything best but ignore the priority of different tasks. Google (pipeline) ... My mentor told me that sometimes we don't have to do everything best. As an intern, I only need to do one project. But as a full time employee. They usually have a lot work to do. What they do is to meet the standard and expectation but not to do everything best. This way they could do more projects and make more impacts than only do one thing best. I think in the future I should have more experience on deciding priority of different tasks.

## Technical

### Data Warehouse

- Data Mining: Data mining is a process of analyzing the data in different dimensions or perspectives and summarizing into a useful information.
- Database: Database is a collection of related data that represents some elements of the real world. Database is designed to record data.
- Data Lake: A data lake is a large repository of raw data, either unstructured or semi-structured. This data is aggregated from various sources and is simply stored. It  is not altered to suit a specific purpose or fit into a particular format.
- Data Warehouse: A data warehouse is an aggregation of data from many sources to a single, centralized repository that unifies the data qualities and format, making it useful for data scientists to use in data mining, artificial intelligence (AI), machine learning and, ultimately, business analytics and business intelligence.
- Data Mart: A data mart is a subset of a data warehouse that benefits a specific set of users within the business or business unit. Data sets within a data mart are often utilized in real time, for current analysis and actionable results.
- OLTP: Online transaction processing (OLTP) captures, stores, and processes data from transactions in real time. 
- OLAP: Online analytical processing (OLAP) uses complex queries to analyze aggregated historical data from OLTP systems
- Fact and dimension tables: A dimension table contains a surrogate key, natural key, and a set of attributes. On the contrary, a fact table contains a foreign key, measurements, and degenerated dimensions. Dimension tables provide descriptive or contextual information for the measurement of a fact table.
- Star Schema: In star schema, The fact tables and the dimension tables are contained. In this schema fewer foreign-key join is used. This schema  forms a star with fact table and dimension tables. Star schema is simple and takes less time for query execution but has high data redundancy

<img src="https://i.loli.net/2021/09/28/vSRgEJDBG5wdHcx.png" alt="image-20210927203958117" style="zoom: 50%;" />

- Snowflake Schema: In snowflake schema, The fact tables, dimension tables as well as sub dimension tables are contained. This schema forms a snowflake with fact tables, dimension tables as well as sub-dimension tables. Snowflake schema is complex and takes more time for execution but has low data redundancy

<img src="https://i.loli.net/2021/09/28/Bjq8p6zh9tOcNol.png" alt="image-20210927204044533" style="zoom: 50%;" />



### Data Transfer

- scp: scp is Secure Copy, which is used to copy a remote file. Data transfer uses ssh method to provide security guarantee.

- rcp: The target host requires the RCP function in advance and sets the RCP permission
- wget: wget is a free tool for automatically downloading files from the Internet,  supports three most common TCP / IP protocols through HTTP, HTTPS, FTP,  and you can use HTTP proxy.
- rsync: Rsync is a data mirror backup tool under UNIX systems, which can be seen from the software name - Remote Sync. Its operational mode and SCP are similar, but much better than SCP. When using a double colon segment the host name and file path, use the RSYNC server.

### Database

Concepts

- PRIMARY KEY: constraint uniquely identifies each row in a table.
- FOREIGN KEY: comprises of single or collection of fields in a table that essentially refers to the PRIMARY KEY in another table.
- Entity: An entity can be a real-world object, either tangible or intangible, that can be easily identifiable.
- Relationships: Relations or links between entities that have something to do with each other. 

Normalization

- First Normal Form(1NF): A relation is in first normal form if every attribute in that relation is a single-valued attribute.
- Second Normal Form(2NF): A relation is in second normal form if it satisfies the conditions for the first  normal form and does not contain any partial dependency.
- Third Normal Form(3NF): A relation is said to be in the third normal form, if it satisfies the conditions for the second normal form and there is no transitive dependency between the non-prime attributes

Different types of join

- `(INNER) JOIN`: Returns records that have matching values in both tables
- `LEFT (OUTER) JOIN`: Returns all records from the left table, and the matched records from the right table
- `RIGHT (OUTER) JOIN`: Returns all records from the right table, and the matched records from the left table
- `FULL (OUTER) JOIN`: Returns all records when there is a match in either left or right table
- `CROSS JOIN`: Returns a paired combination of each row of the first table with each row of the second table

Order of SQL

FROM-ON-JOIN-WHERE-GROUP BY-HAVING-SELECT-DISTINCT-ORDER BY-LIMIT

Tune SQL

- Define business requirements first: identify relevant stakeholders and focus on business outcomes
- SELECT fields instead of using SELECT *: only pull the required information
- Avoid SELECT DISTINCT: SELECT DISTINCT works by group all fields in the query
- Create joins with inner join but not where: where clause used cross join and then filter all the records
- Use where instead of having to define filters: having are calculated after where statements, in comparison where limit the number of records
- Use wildcards at the end of a phrase only: wildcards create the widest search possible and are inefficient
- Use LIMIT to sample query results: use limit to return number of records specified
- Run your query during off-peak hours

Timestamp

- CURDATE()
- CURTIME()

- DATE_ADD(date,INTERVAL expr)
- DATE_SUB(date,INTERVAL expr)
- DATEDIFF(date1, date2)
- TIMEDIFF(time1, time2)

String

- LOWER()
- UPPER()
- SUBSTR(str, pos, [,len])
- LENGTH(str)
- REPLACE(str, from_str, to_str)
- STRCMP(expr1, expr2)

ACID property

- **Atomicity**: This property ensures that the transaction is completed in all-or-nothing way.
- **Consistency**: This ensures that updates made to the database is valid and follows rules and restrictions.
- **Isolation**: This property ensures integrity of transaction that are visible to all other transactions.
- **Durability**: This property ensures that the committed transactions are stored permanently in the database.

MVCC: MVCC or Multi-version concurrency control is used for avoiding unnecessary database locks when 2 or more requests tries to access or modify the data at the same time.

- **Dirty reads**: If a transaction reads data that is written due to concurrent uncommitted transaction, these reads are called dirty reads.
- **Phantom reads**: This occurs when two same queries when  executed separately return different rows. For example, if transaction A retrieves some set of rows matching search criteria. Assume another  transaction B retrieves new rows in addition to the rows obtained  earlier for the same search criteria. The results are different.
- **Non-repeatable reads**: This occurs when a transaction  tries to read the same row multiple times and gets different values each time due to concurrency. This happens when another transaction updates  that data and our current transaction fetches that updated data,  resulting in different values.

To tackle these, there are 4 standard isolation levels defined by SQL standards. They are as follows:

- **Read Uncommitted** – The lowest level of the isolations.  Here, the transactions are not isolated and can read data that are not  committed by other transactions resulting in dirty reads.
- **Read Committed** – This level ensures that the data read is committed at any instant of read time. Hence, dirty reads are avoided here. This level makes use of read/write lock on the current rows which prevents read/write/update/delete of that row when the current  transaction is being operated on.
- **Repeatable Read** – The most restrictive level of  isolation. This holds read and write locks for all rows it operates on.  Due to this, non-repeatable reads are avoided as other transactions  cannot read, write, update or delete the rows.
- **Serializable** – The highest of all isolation levels.  This guarantees that the execution is serializable where execution of  any concurrent operations are guaranteed to be appeared as executing serially.

